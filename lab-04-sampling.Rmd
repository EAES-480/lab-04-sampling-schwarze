---
title: "Lab 04 — Sampling from Time Series"
author: "EAES 480 — Modern Statistics in Earth & Environmental Science"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

# Overview

## What is AmeriFlux?

**AmeriFlux** is a network of **eddy covariance** flux-tower sites that measure exchanges of **carbon (CO₂), water, and energy** between ecosystems and the atmosphere, with standardized data products shared for research and education.

In this lab, you will use a simplified AmeriFlux-style dataset from the **US-AMS site at Argonne National Laboratory (near Chicago)**. The measurements are at **30-minute resolution** over **2023**, and show strong **seasonality** and **day–night cycles**.

**Key idea for this lab:** treat the full 2023 time series as the **population**, then practice sampling strategies to estimate population parameters.

References for context:
- AmeriFlux overview: https://ameriflux.lbl.gov/about/about-ameriflux/
- US-AMS site page: https://ameriflux.lbl.gov/sites/siteinfo/US-AMS

---

# Learning goals

By the end of this lab, you should be able to:

- Define a **population** and a **sample** for an EAES time-series dataset
- Compute population **parameters** (mean, SD) and compare to sample **estimates**
- Visualize distributions and identify **latent grouping variables** (month, day/night)
- Implement **simple random sampling** and **stratified sampling**
- Use `set.seed()` to make sampling reproducible

---

# Data

## Load and inspect

```{r load_data, echo=TRUE, eval=FALSE}
df <- read_csv("data/us-ams-simple.csv") %>%
  clean_names() %>% filter(year_local == 2023)

glimpse(df)
```

**CHECK:** You should see columns like `year_local`, `doy`, `daytime`, and flux/biomet variables (e.g., `gpp`, `fc`, `le`, `ta`).

---

## Create a date and month column from DOY

This dataset uses **Year + Day-of-Year (DOY)**. Month must be derived from a calendar date.

```{r derive_time, echo=TRUE, eval=FALSE}
df <- df %>%
  mutate(
    # TODO: create a Date column from year_local and doy
    # HINT: Jan 1 is DOY = 1, so use (doy - 1) with origin = "YYYY-01-01"
    date = as.Date(doy - 1, origin = paste0(year_local, "-01-01")),

    # TODO: create a month column (numeric 1–12 or labeled months)
    month = month(date, label = TRUE, abbr = TRUE),

    # TODO: make a day/night label using daytime (0/1)
    day_night = if_else(daytime == 1, "Day", "Night")
  )

count(df, month)
count(df, day_night)
```

---

# Choose a response variable

You will analyze **one response variable** throughout the lab. This could be a CO₂ flux metric or a meteorological variable.

Examples you can choose from (depending on what you see in the dataset):
- CO₂ / carbon: `gpp`, `reco`, `fc`
- Energy: `le`
- Meteorology: `ta`, `ts`, `swc`

```{r choose_response, echo=TRUE, eval=FALSE}
# TODO: choose ONE response variable (a column name as a string)
response_var <- "reco"   # replace "gpp" with your choice, e.g. "fc" or "le" or "ta"

# CHECK: print a quick summary
df %>% summarise(
  n = n(),
  n_missing = sum(is.na(reco)),
  mean = mean(reco, na.rm = TRUE),
  sd = sd(reco, na.rm = TRUE)
)
```

**Prompt (2–3 sentences):** Why did you choose this response variable? What do you expect its seasonality/day–night pattern to be?

> I chose this because environmental respiration is important in cycling carbon from the atmosphere. I would expect it to be higher in the summer and lower in the winter since there are less plants alive during the winter. I would also expect it to be higher during the day since a lot of plants require solar radiation to be productive. 

---

# Section 1 — Data dictionary (conceptual)

Students will populate the data dictionary using:
https://ameriflux.lbl.gov/data/aboutdata/data-variables/

Fill in at least **5 variables** from this dataset:

| Variable | Units | Description | Expected sign/seasonality? |
|----------|-------|-------------|----------------------------|
| ta   |  deg C   |     air temperature      |     daily and seasonal fluctuations                 |
|     reco     |   µmolCO2 m-2 s-1    |      ecosystem respiration       |            daily and seasonal fluctuations          |
|    GPP      |   µmolCO2 m-2 s-1    |     Gross Primary Productivity      |            daily and seasonal fluctuations          |
|     fc     |    µmolCO2 m-2 s-1   |     Carbon Dioxide (CO2) turbulent flux (no storage correction        |         daily and seasonal fluctuations         |
|     le     |   W m-2    |     latent heat        |    daily and seasonal fluctuations, changes with precipitation and evaporation  |

---

# Section 2 — Visualizing the population

Remember: for this lab, the **population** is the entire 2023 half-hourly time series.

## 2.1 Time series view

```{r plot_time_series, echo=TRUE, eval=FALSE}
# GOAL: Visualize seasonality over the year.
# TODO: pick a y aesthetic using response_var.

df %>%
  filter(year(date) == 2023) %>%
  ggplot(aes(x = date, y = reco)) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(
    x = NULL,
    y = response_var,
    title = "Population time series (2023)"
  )
```

**Prompt (2–3 sentences):** What major patterns do you see? (Seasonal cycle? Daily cycle? Outliers?)

> I see a big spike in the warmer months between April and October, which is where summer would fall. There are also small up-and-down spikes that represent daily cycling. 

---

## 2.2 Population distribution (histogram + density)

```{r pop_distribution, echo=TRUE, eval=FALSE}
# GOAL: See the overall distribution of the population.
# TODO: choose an appropriate number of bins (start with ~50).

ggplot(df, aes(x = reco)) +
  geom_histogram(bins = 50, alpha = 0.7, fill = "palegreen4", color = "palegreen4") +
  theme_classic(base_size = 18) +
  labs(x = response_var, y = "Count", title = "Population distribution (histogram)")

ggplot(df, aes(x = reco)) +
  geom_density(alpha = 0.7) +
  theme_classic(base_size = 18) +
  labs(x = response_var, y = "Density", title = "Population distribution (density)")
```

**Prompt:** Describe shape (skew, modality), center, and spread.

> The distribution is very skewed to the right. The center of the data would be closer to the right since the density is much higher in between 0 and 5, which alos means the spread is most condensed around these values. 

---

## 2.3 Do latent groups explain variability? (month, day/night)

### By month

```{r pop_by_month, echo=TRUE, eval=FALSE}
# TODO: make month appear in a sensible order (it already is an ordered factor if label=TRUE)
df %>%
  ggplot(aes(x = month, y = reco)) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by month")
```

### By day/night

```{r pop_by_daynight, echo=TRUE, eval=FALSE}
df %>%
  ggplot(aes(x = day_night, y = reco)) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by day vs night")
```

**Prompt (3–4 sentences):** Which grouping variable (month or day/night) seems to explain more variability in your response? Why?

> The grouping by month shows a lot more variablity in the model. While there is some variation from day to night, there is a lore more variation across the months. The sizes of the boxes and the whiskers are a lot more variable compared to each other than the day and night plot. 

---

# Section 3 — Population parameters (truth)

Compute the population mean and SD for your chosen response variable.

```{r population_params, echo=TRUE, eval=FALSE}
pop_mean <- mean(df$reco, na.rm = TRUE)
pop_sd   <- sd(df$reco, na.rm = TRUE)

tibble(
  response_var = response_var,
  population_mean = pop_mean,
  population_sd = pop_sd
)
```

---

# Section 4 — Simple random sampling (SRS)

## 4.1 One random sample

```{r one_sample, echo=TRUE, eval=FALSE}
set.seed(480)

# TODO: choose a sample size (e.g., 200, 500, 1000)
n_samp <- 1000

samp <- df %>%
  slice_sample(n = n_samp)

samp_mean <- mean(samp$reco, na.rm = TRUE)
samp_sd   <- sd(samp$reco, na.rm = TRUE)

tibble(
  n_samp = n_samp,
  sample_mean = samp_mean,
  sample_sd = samp_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

**Prompt (2–3 sentences):** How close is your one-sample estimate to the population mean/SD? Is the difference surprising?

> The sample estimate is very close to the population mean, and not very far from the standard deviation either. Both are within 0.07 and 0.2 of each other respectively, which is not a very big difference. This does not necessarily surprise me since the sample should be pretty representative of the entire dataset. 

---

## 4.2 Sampling variability: many samples → many means

```{r sampling_distribution, echo=TRUE, eval=FALSE}
set.seed(480)

reps <- 1000   # TODO: choose number of replicates (e.g., 500 or 1000)

means <- replicate(
  reps,
  df %>%
    slice_sample(n = n_samp) %>%
    summarise(m = mean(reco, na.rm = TRUE)) %>%
    pull(m)
)

ggplot(tibble(mean_est = means), aes(mean_est)) +
  geom_histogram(bins = 40, alpha = 0.8) +
  geom_vline(xintercept = pop_mean, linetype = "dashed", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = paste0("Sample mean of ", response_var),
    y = "Count",
    title = "Sampling distribution of the mean (SRS)",
    subtitle = "Dashed line = population mean"
  )
```

**Prompt (2–3 sentences):** Is the sampling distribution centered on the population mean? What happens if you increase `n_samp`?

> The sampling distribution is centered near the population mean, but looks to be possibly be just under the mean value. The sample mean is around 6.4, but the center of the distribution looks a bit too close to 6.3. If you increase the value of `n_samp`, the center seems to be more centered on the mean value. 

---

# Section 5 — Stratified sampling

Here you’ll test whether stratification helps when the population has structure.

## 5.1 Stratify by month

```{r strat_by_month, echo=TRUE, eval=FALSE}
set.seed(480)

# GOAL: sample within each month to ensure seasonal representation.
# TODO: choose n_per_month so total sample size is reasonable (e.g., 12 * 20 = 240)
n_per_month <- 20

samp_strat <- df %>%
  group_by(month) %>%
  slice_sample(n = n_per_month) %>%
  ungroup()

strat_mean <- mean(samp_strat$reco, na.rm = TRUE)
strat_sd   <- sd(samp_strat$reco, na.rm = TRUE)

tibble(
  n_per_month = n_per_month,
  total_n = nrow(samp_strat),
  strat_mean = strat_mean,
  strat_sd = strat_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

---

## 5.2 Compare strategies (SRS vs stratified)

```{r compare_sampling, echo=TRUE, eval=FALSE}
tibble(
  strategy = c("Population", "SRS", "Stratified by month"),
  mean = c(pop_mean, samp_mean, strat_mean),
  sd   = c(pop_sd,   samp_sd,   strat_sd)
)
```

**Prompt (3–4 sentences):** Which strategy better approximated the population mean and SD for your response variable? Why might stratification help (or not) here?

> The stratified stratgey approximates the data better than SRS, but the difference is not too staggering. The mean value is a lot closer to the population mean, but the difference between the standard deviation is about the same as when using SRS. Stratification might help because the subgrouping of stratificiation might account for the natural distribution of plants more accurately than SRS. Since most organisms follow a clustered pattern than a uniform one, SRS may be selecting some areas with fewer respirating plants than stratified sampling would since it separates according to plant groupings.  

---

# Section 6 — Conceptual reflection

Answer in **4–6 sentences**:

- Why does seasonality matter for sampling?
- What happens if sampling ignores latent grouping variables?
- In EAES field studies, when is stratification essential?
- What is one trade-off of stratified sampling?

> Seasonality matters for sampling due to the differences in solar radiation and precipitation, in which a lot of Earth's functions are influenced. If sampling ignores latent grouping variables, it is likely data will be biased since it doesn't account for the natural heterogeneity that exists amongst variables. Stratification in essential in EaES studies because it acknowledges latent grouping variables and divides populations accordingly to get a better representation of them. One trade-off of stratified sampling might be more expensive and time-intensive than random sampling since there is the extra effort of splitting up the population rather than choosing random points. 

---

# Part II — Sampling designs extensions (graded practice)

In Part I you treated the full 2023 half-hourly record as a **population**, then compared **simple random sampling** (SRS) vs **stratified sampling by month**.

Now you will practice additional **sampling designs** discussed in lecture:

- **Systematic** sampling (regular interval)
- **Cluster** sampling (sample groups, then measure everything in them)
- **Quasi-continuous** sampling (regular time series subsampling)
- **Blocked** designs (preview only — think “blocks as structure”)

All exercises below should run using the same objects from Part I:
- `df` (with `date`, `month`, `day_night`)
- `response_var`
- `pop_mean`, `pop_sd`
- your SRS sample `samp` and stratified sample `samp_strat` (if you created them)

> **Tip:** If you renamed objects in Part I, update the code below to match your names.

---

## Exercise 1 — Systematic sampling (every k-th observation)

**Idea:** sample at a fixed interval (e.g., every 48th record ≈ daily at 30-min resolution).

**Risk:** if the variable has strong cycles aligned with the interval, systematic sampling can be biased.

```{r systematic_sampling, echo=TRUE, eval=FALSE}
# GOAL: Create a systematic sample and compare mean/sd to population.
# TODO: choose an interval k (try 48, 24, 96).
k <- 48

sys_samp <- df %>%
  # TODO: keep only non-missing response values
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = k))

sys_mean <- mean(sys_samp[[response_var]], na.rm = TRUE)
sys_sd   <- sd(sys_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Systematic"),
  mean = c(pop_mean, sys_mean),
  sd   = c(pop_sd,   sys_sd),
  n    = c(nrow(df), nrow(sys_samp))
)
```

**Prompt (3–4 sentences):** Did systematic sampling approximate the population mean/SD better or worse than SRS?  
What cycle (daily or seasonal) might be interacting with your chosen `k`?

> The systematic sampling approximated the population mean and SD significantly worse than SRS. The difference between the population mean is 0.6, and the SD 0.5, which is the largest margin of difference between any of the sampling methods so far. I think the daily cycle might be interacting with the `k` value since k represents the interval between chosen values and a k of 48 means you are sampling every 48th hour. This means some patterns of the daily cycle will be skipped since 48 hours would be spanning across two days.

---

## Exercise 2 — Cluster sampling (sample days, take all points within those days)

**Definition:** choose a set of clusters (here, **days**) at random, then include **all observations** in the chosen clusters.

This mimics EAES logistics: you may only be able to sample on certain days.

```{r cluster_sampling_days, echo=TRUE, eval=FALSE}
# GOAL: Sample whole days as clusters, then estimate mean/sd.
# TODO: choose number of days to sample.
set.seed(480)

n_days <- 150

days <- df %>%
  distinct(date) %>%
  drop_na(date) %>%
  pull(date)

chosen_days <- sample(days, size = n_days, replace = FALSE)

cluster_samp <- df %>%
  filter(date %in% chosen_days) %>%
  filter(!is.na(.data[[response_var]]))

clust_mean <- mean(cluster_samp[[response_var]], na.rm = TRUE)
clust_sd   <- sd(cluster_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Cluster (days)"),
  mean = c(pop_mean, clust_mean),
  sd   = c(pop_sd,   clust_sd),
  n    = c(nrow(df), nrow(cluster_samp))
)
```

**Prompt (3–4 sentences):** Why might cluster sampling have **higher variance** than SRS for the same number of measurements?  
What feature of time series data (hint: autocorrelation) is relevant here?

> In this case, the cluster sampling did not have higher variance than SRS, and was actually the most representative method used so far. However, it may show more variance in the case of multiple clusters where data varies greatly per cluster, providing a skewed interpretation of data. The feature relevant here is autocorrelation because the homogeneity would be autocorrelated within clusters and intensify the inaccuracy. 

---

## Exercise 3 — Quasi-continuous sampling (fixed schedule time series)

**Definition:** sample regularly over time to create a *subsampled time series*.

This mimics continuous instrumentation that logs at a lower frequency (e.g., hourly instead of 30-min).

```{r quasi_continuous, echo=TRUE, eval=FALSE}
# GOAL: Create a regular subsampled time series.
# TODO: choose a step size (every 2 records = hourly; every 4 = 2-hourly).
step <- 2

qc_samp <- df %>%
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = step))

qc_mean <- mean(qc_samp[[response_var]], na.rm = TRUE)
qc_sd   <- sd(qc_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Quasi-continuous"),
  mean = c(pop_mean, qc_mean),
  sd   = c(pop_sd,   qc_sd),
  n    = c(nrow(df), nrow(qc_samp))
)
```

### Visual check: does the subsampled series preserve structure?

```{r qc_plot, echo=TRUE, eval=FALSE}
# TODO: plot BOTH population and qc_samp time series (thin lines) for a short window
# HINT: filter to one month to avoid overplotting.

df %>%
  filter(month == "Jul") %>%   # e.g., "Jul" if month is labeled
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(title = "Population time series (subset)", x = NULL, y = response_var)

qc_samp %>%
  filter(month == "Jul") %>%
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.6) +
  theme_classic(base_size = 18) +
  labs(title = "Quasi-continuous sample time series (subset)", x = NULL, y = response_var)
```

**Prompt (3–4 sentences):** Does quasi-continuous sampling preserve the **seasonal** pattern? The **daily** pattern?  
In an EAES context, when is quasi-continuous sampling preferable to sparse discrete sampling?

> The quasi-continuous sampling preserves both the seaonal and daily patters, as indicated by the small undulations in the data (daily) and the overarching curve as the month progresses (seasonal). Quasi-continuous sampling may be preferable to sparse discrete sampling in an EaES context because more data is gathered and is therfore more likely to demonstrate naturally occuring, non-linear trends. Continuing a high volume of data over a longer period of time is more likely to include longterm events across seasons, or even across several years, such as large flood events or climate variance. 

---

## Exercise 4 — Compare all strategies in one table (and interpret)

```{r compare_all, echo=TRUE, eval=FALSE}
# GOAL: Assemble a comparison table of mean/sd error for each strategy.
# NOTE: This assumes you created objects: samp, samp_strat, sys_samp, cluster_samp, qc_samp.
# If your object names differ, update them here.

summ_stats <- function(dat, label) {
  tibble(
    strategy = label,
    n = nrow(dat),
    mean = mean(dat[[response_var]], na.rm = TRUE),
    sd   = sd(dat[[response_var]], na.rm = TRUE)
  )
}

bind_rows(
  tibble(strategy = "Population", n = nrow(df), mean = pop_mean, sd = pop_sd),
  summ_stats(samp, "SRS"),
  summ_stats(samp_strat, "Stratified (month)"),
  summ_stats(sys_samp, "Systematic"),
  summ_stats(cluster_samp, "Cluster (days)"),
  summ_stats(qc_samp, "Quasi-continuous")
) %>%
  mutate(
    mean_error = mean - pop_mean,
    sd_error   = sd - pop_sd
  )
```

**Prompt (5–6 sentences, graded):** Which strategy gave the best estimate of the population mean? Of the population SD?  
Explain *why* in terms of (i) seasonal/diurnal structure and (ii) dependence/autocorrelation.  
If you were designing a real EAES study with limited field days, what hybrid strategy would you propose (e.g., stratified + clustered)?

> The stratgey that gave the best estimate of the population mean was the cluster method, and the one that gave the best estimate of the SD was quasi-continuous. I think the cluster method provided a good estimate of the population mean because it targeted the areas where more plants were present, likely as a result of plants' clumped behavior patterns. In terms of standard deviation, I think the quasi-continuous method did a good job because it collected data over a longer period of time that better encapsulated the seasonality changes demonstrated in the population data. If I were desigining a real EaES study with a limited amount of time, I would choose to use a stratified method paired with systematic sampling. This is because the systematic design would be demonstrative of possible gradients in data, illustrating trends, while the stratified aspect would target areas of interest, eliminating areas that would possibly introduce unnecessary outliers. 

---

## Blocked designs (preview only — do not implement yet)

A **blocked** design means sampling within blocks (time blocks like months, or spatial blocks like sites), then later treating block as structure in the model (often as a random effect).

You already approximated blocking by stratifying over `month`. Later, we will return to this idea when we fit models that include block structure explicitly.

# Submission + self-check

## Before you knit (Run All)
- [ ] Setup chunk runs without errors
- [ ] Data join happened (df_raw has both canopy + climate columns)
- [ ] All chunks run top-to-bottom
- [ ] No objects created only in the Console
- [ ] Interpretation answers are complete sentences

## After you knit
- [ ] Figures appear in the output
- [ ] Tables render and are readable
- [ ] Your selected x and y are clearly stated in the document

**Save, commit, push to Github.**
